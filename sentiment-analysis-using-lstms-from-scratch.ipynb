{"cells":[{"metadata":{},"cell_type":"markdown","source":"References:https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948\n           https://medium.com/@bhadreshpsavani/tutorial-on-sentimental-analysis-using-pytorch-b1431306a2d7"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":171,"outputs":[{"output_type":"stream","text":"/kaggle/input/reviews/reviews1.txt\n/kaggle/input/moviereviews/labels.txt\n/kaggle/input/moviereviews/reviews.txt\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport csv\nfrom string import punctuation\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\n","execution_count":286,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n# read data from text files\nwith open('../input/moviereviews/labels.txt', 'r') as f:\n     labels = f.readlines()\nwith open('../input/reviews/reviews1.txt', 'r') as f:\n     reviews = f.readlines()","execution_count":240,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_reviews=list()\nfor text in reviews:\n  text = text.lower()\n  text = \"\".join([ch for ch in text if ch not in punctuation])\n  all_reviews.append(text)\nall_text = \" \".join(all_reviews)\nall_words = all_text.split()","execution_count":241,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(reviews)","execution_count":242,"outputs":[{"output_type":"execute_result","execution_count":242,"data":{"text/plain":"25000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter \n# Count all the words using Counter Method\ncount_words = Counter(all_words)\ntotal_words=len(all_words)\nsorted_words=count_words.most_common(total_words)\n","execution_count":243,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_to_int={}\nfor i, (word,_) in enumerate(sorted_words):\n    word_to_int[word]=i+1","execution_count":244,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encodeReview(review):\n    encodedReview =[]\n    for word in review.split():\n        if word not in word_to_int.keys():\n            encodedReview.append(0)\n        else:\n            encodedReview.append(word_to_int[word])\n    return encodedReview","execution_count":245,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encodedReviews=list()\nfor review in all_reviews:\n    encodedReviews.append(encodeReview(review))","execution_count":246,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviewsLen = [len(x) for x in encodedReviews]\npd.Series(reviewsLen).hist()","execution_count":250,"outputs":[{"output_type":"execute_result","execution_count":250,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff6c52a8550>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(reviewsLen).describe()","execution_count":251,"outputs":[{"output_type":"execute_result","execution_count":251,"data":{"text/plain":"count    25000.00000\nmean       240.80784\nstd        179.01773\nmin         10.00000\n25%        130.00000\n50%        179.00000\n75%        293.00000\nmax       2514.00000\ndtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_inputs(encodedReviews,maxLength=280 ):\n    X =np.zeros((len(encodedReviews), maxLength), dtype=int) \n    for i, encodedReview in enumerate(encodedReviews):\n        if len(encodedReview) <=maxLength:\n            encodedReview = [0]*(maxLength-len(encodedReview))+encodedReview \n        else:\n            encodedReview=encodedReview[:maxLength]\n        X[i,:] = np.array(encodedReview)\n    return X","execution_count":271,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=pad_inputs(encodedReviews)","execution_count":272,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels =[1 if i=='positive\\n' else 0  for i in labels]\nlabels= np.array(labels)","execution_count":274,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= encodedReviews\ny= labels","execution_count":275,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1) # 0.1 x 0.9 = 0.09","execution_count":276,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train), len(y_train)","execution_count":277,"outputs":[{"output_type":"execute_result","execution_count":277,"data":{"text/plain":"(16000, 16000)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\nvalid_data=TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\ntest_data=TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))","execution_count":278,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataloader\nbatch_size=50\ntrain_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\ntest_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)","execution_count":279,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X_train[0])","execution_count":280,"outputs":[{"output_type":"execute_result","execution_count":280,"data":{"text/plain":"280"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","execution_count":281,"outputs":[{"output_type":"stream","text":"Sample input size:  torch.Size([50, 280])\nSample input: \n tensor([[8.0000e+00, 1.3000e+01, 8.6100e+02,  ..., 4.3000e+01, 1.0650e+03,\n         3.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3600e+02, 8.1100e+02,\n         8.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5790e+03, 1.0000e+00,\n         1.8700e+02],\n        ...,\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0340e+03, 7.0000e+00,\n         7.0000e+00],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8920e+03, 4.3130e+03,\n         1.2000e+02],\n        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6000e+01, 5.0000e+00,\n         2.9000e+01]])\n\nSample label size:  torch.Size([50])\nSample label: \n tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0.])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMSentiment(nn.Module):\n    \n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \n        super().__init__()\n        \n        self.output_size=output_size\n        self.n_layers=n_layers\n        self.hidden_dim=hidden_dim\n        \n        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        self.dropout=nn.Dropout(0.3)\n        \n        self.fc1=nn.Linear(hidden_dim, 64)\n        self.fc2=nn.Linear(64, 16)\n        self.fc3=nn.Linear(32, 8)\n        self.fc4=nn.Linear(32,output_size)\n        self.sigmoid=nn.Sigmoid()\n        \n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        embedding =self.embedding(x)\n        out, hidden = self.lstm(embedding, hidden)\n        out=out.contiguous().view(-1, self.hidden_dim)\n        out=self.dropout(lstm_out)\n        \n        out=self.fc1(out)\n        out=self.dropout(out)\n        \n        out=self.fc2(out)\n        out=self.dropout(out)\n        \n        out=self.fc3(out)\n        out=self.dropout(out)\n        \n        out=self.fc4(out)\n        out=self.dropout(out)\n        \n        out=self.sigmoid(out)\n        \n        out=out.view(batch_size, -1)\n        out=out[:, -1]\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        \"\"\"Initialize Hidden STATE\"\"\"\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":331,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass SentimentLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super().__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n        ","execution_count":365,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_to_int)+1 \noutput_size = 1\nembedding_dim = 300\nhidden_dim = 256\nn_layers = 2","execution_count":366,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(net)","execution_count":367,"outputs":[{"output_type":"stream","text":"SentimentLSTM(\n  (embedding): Embedding(74073, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nepochs = 4\n\ntrain_on_gpu = torch.cuda.is_available()\n","execution_count":368,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, param in net.named_parameters():\n    if param.requires_grad:\n        print(name)","execution_count":369,"outputs":[{"output_type":"stream","text":"embedding.weight\nlstm.weight_ih_l0\nlstm.weight_hh_l0\nlstm.bias_ih_l0\nlstm.bias_hh_l0\nlstm.weight_ih_l1\nlstm.weight_hh_l1\nlstm.bias_ih_l1\nlstm.bias_hh_l1\nfc.weight\nfc.bias\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.parameters()","execution_count":370,"outputs":[{"output_type":"execute_result","execution_count":370,"data":{"text/plain":"<generator object Module.parameters at 0x7ff6ac6aba50>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\nprint_every = 100\nclip=5 # gradient clipping","execution_count":371,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if(train_on_gpu):\n    net.cuda()\n    \nnet.train()","execution_count":372,"outputs":[{"output_type":"execute_result","execution_count":372,"data":{"text/plain":"SentimentLSTM(\n  (embedding): Embedding(74073, 300)\n  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n  (sig): Sigmoid()\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n\n# training params\n\nepochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        inputs = inputs.type(torch.LongTensor)\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                inputs = inputs.type(torch.LongTensor)\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":373,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-373-edd2e9938a75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# get the output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-365-cbfa3f1c917a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# embeddings and lstm_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #3 'index' in call to _th_index_select"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}